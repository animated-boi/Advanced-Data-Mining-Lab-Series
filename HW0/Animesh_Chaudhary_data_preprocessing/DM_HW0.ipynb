{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWxkNnWrvHEb",
        "outputId": "ae64c3bd-5b85-40e5-d51e-7af691c007ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "#Setting up Environment\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Download the punkt_tab model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading dictionary\n",
        "dictionary = set()\n",
        "with open('dictionary.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        dictionary.add(line.strip())\n",
        "print(f\"Loaded {len(dictionary)} words into the dictionary.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYgDV71yvfSJ",
        "outputId": "881e68cf-a8c7-46e0-8406-7c72a95d1de1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1000 words into the dictionary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text, dictionary):\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    processed_words = [stemmer.stem(word) for word in filtered_tokens if word in dictionary]\n",
        "\n",
        "    return processed_words\n"
      ],
      "metadata": {
        "id": "Q_O99c9zv3hM"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Testing\n",
        "sample_text = \"The quick brown fox buy jumps apple over the  16 loss lazy dog.  campaign,, Foxes are faster than 50 dogs!\"\n",
        "preprocessed_words = preprocess_text(sample_text, dictionary)\n",
        "print(\"Preprocessed Words:\", preprocessed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wjPyFm6wD8y",
        "outputId": "f8dfe826-81b8-4dda-8b83-8f8fe22323d4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed Words: ['brown', 'buy', '16', 'loss', 'campaign', '50']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "dataset = pd.read_csv('24_train_3.csv')\n",
        "\n",
        "print(dataset.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnNrUhJexu8w",
        "outputId": "b8331d52-2197-4880-8070-689bada163b1"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ArticleId                                               Text  Category\n",
            "0       1429  sfa awaits report over mikoliunas the scottish...     sport\n",
            "1       1896  parmalat to return to stockmarket parmalat  th...  business\n",
            "2       1633  edu blasts arsenal arsenal s brazilian midfiel...     sport\n",
            "3       2178  henman decides to quit davis cup tim henman ha...     sport\n",
            "4        194  french suitor holds lse meeting european stock...  business\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying preprocessing to the 'Text'\n",
        "dataset['Preprocessed_Text'] = dataset['Text'].apply(lambda x: preprocess_text(x, dictionary))\n",
        "\n",
        "print(dataset[['ArticleId', 'Category', 'Preprocessed_Text']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNLzCEoWx9ss",
        "outputId": "94e0d972-928e-4509-9e86-fa0ad3ed30ae"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ArticleId  Category                                  Preprocessed_Text\n",
            "0       1429     sport  [report, scottish, report, 20, award, defeat, ...\n",
            "1       1896  business  [return, went, back, stock, firm, 2003, eight,...\n",
            "2       1633     sport  [hit, club, new, contract, deal, next, summer,...\n",
            "3       2178     sport  [quit, cup, great, britain, cup, team, made, c...\n",
            "4        194  business  [french, european, stock, market, met, london,...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the preprocessed data\n",
        "dataset.to_csv('preprocessed_dataset.csv', index=False)\n"
      ],
      "metadata": {
        "id": "gTuE3PWqyD6m"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define TFIDF Functions"
      ],
      "metadata": {
        "id": "WULY_aIxyRNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute Term Frequency (TF):\n"
      ],
      "metadata": {
        "id": "MrZZ9f0ByVhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def compute_tf(word_list):\n",
        "    \"\"\"\n",
        "    Compute term frequency for a document.\n",
        "    Args:\n",
        "        word_list (list): List of preprocessed words in a document.\n",
        "    Returns:\n",
        "        dict: Dictionary of word -> TF value.\n",
        "    \"\"\"\n",
        "    term_count = Counter(word_list)\n",
        "    max_count = max(term_count.values())\n",
        "    tf = {word: count / max_count for word, count in term_count.items()}\n",
        "    return tf\n"
      ],
      "metadata": {
        "id": "WIPa7Wn3yR9h"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute Inverse Document Frequency (IDF):"
      ],
      "metadata": {
        "id": "wPx9UuLGycli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_idf(documents):\n",
        "    \"\"\"\n",
        "    Compute inverse document frequency for all words.\n",
        "    Args:\n",
        "        documents (list): List of lists, where each sublist contains words in a document.\n",
        "    Returns:\n",
        "        dict: Dictionary of word -> IDF value.\n",
        "    \"\"\"\n",
        "    n = len(documents)\n",
        "    word_doc_count = Counter()\n",
        "    for doc in documents:\n",
        "        unique_words = set(doc)\n",
        "        for word in unique_words:\n",
        "            word_doc_count[word] += 1\n",
        "    idf = {word: np.log10(n / count) for word, count in word_doc_count.items()}\n",
        "    return idf\n"
      ],
      "metadata": {
        "id": "Ss9Fw7N_ydJs"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute TFIDF Matrix:"
      ],
      "metadata": {
        "id": "M4jh5O_Jyf7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tfidf(documents, dictionary):\n",
        "    \"\"\"\n",
        "    Compute the TFIDF matrix for all documents.\n",
        "    Args:\n",
        "        documents (list): List of preprocessed documents.\n",
        "        dictionary (list): List of words from the dictionary.txt in order.\n",
        "    Returns:\n",
        "        np.array: TFIDF matrix with shape (num_documents, num_words).\n",
        "    \"\"\"\n",
        "    idf = compute_idf(documents)\n",
        "    tfidf_matrix = []\n",
        "\n",
        "    for doc in documents:\n",
        "        tf = compute_tf(doc)\n",
        "        row = [round(tf.get(word, 0) * idf.get(word, 0), 4) for word in dictionary]\n",
        "        tfidf_matrix.append(row)\n",
        "\n",
        "    return np.array(tfidf_matrix)\n"
      ],
      "metadata": {
        "id": "hVVBS8aiyiGn"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the TFIDF Matrix"
      ],
      "metadata": {
        "id": "vjVEaD9xynyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract preprocessed documents as a list\n",
        "documents = dataset['Preprocessed_Text'].tolist()\n",
        "\n",
        "dictionary_list = sorted(dictionary)\n",
        "\n",
        "# Compute TFIDF matrix\n",
        "tfidf_matrix = compute_tfidf(documents, dictionary_list)\n",
        "\n",
        "# Saving the matrix to a file\n",
        "np.savetxt('matrix.txt', tfidf_matrix, delimiter=',', fmt='%.4f')\n"
      ],
      "metadata": {
        "id": "MrwWpuf3yplB"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frequency Analysis"
      ],
      "metadata": {
        "id": "pwGw0jfgy32m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def compute_word_frequencies_by_category(dataset):\n",
        "    \"\"\"\n",
        "    Compute word frequencies for each category.\n",
        "    Args:\n",
        "        dataset (pd.DataFrame): DataFrame with columns 'Category' and 'Preprocessed_Text'.\n",
        "    Returns:\n",
        "        dict: A dictionary mapping categories to word frequencies.\n",
        "    \"\"\"\n",
        "    category_frequencies = defaultdict(Counter)\n",
        "\n",
        "    for _, row in dataset.iterrows():\n",
        "        category = row['Category']\n",
        "        words = row['Preprocessed_Text']\n",
        "        category_frequencies[category].update(words)\n",
        "\n",
        "    return category_frequencies\n",
        "\n",
        "# Computing word frequencies\n",
        "word_frequencies_by_category = compute_word_frequencies_by_category(dataset)\n",
        "\n",
        "# Extracting top 3 most frequent words for each category\n",
        "top_frequent_words = {\n",
        "    category: dict(frequencies.most_common(3))\n",
        "    for category, frequencies in word_frequencies_by_category.items()\n",
        "}\n",
        "\n",
        "# Saving to JSON\n",
        "import json\n",
        "with open('frequency.json', 'w') as f:\n",
        "    json.dump(top_frequent_words, f, indent=4)\n"
      ],
      "metadata": {
        "id": "i3F1UWmiy4cB"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TFIDF Analysis"
      ],
      "metadata": {
        "id": "7jHn4X6zzB__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tfidf_scores_by_category(tfidf_matrix, dataset, dictionary_list):\n",
        "    \"\"\"\n",
        "    Compute average TFIDF scores for each word in each category.\n",
        "    Args:\n",
        "        tfidf_matrix (np.array): The TFIDF matrix.\n",
        "        dataset (pd.DataFrame): DataFrame with 'Category'.\n",
        "        dictionary_list (list): List of words in the dictionary.\n",
        "    Returns:\n",
        "        dict: A dictionary mapping categories to top TFIDF scores.\n",
        "    \"\"\"\n",
        "    # Mapping categories to their document indices\n",
        "    category_indices = defaultdict(list)\n",
        "    for idx, category in enumerate(dataset['Category']):\n",
        "        category_indices[category].append(idx)\n",
        "\n",
        "    # Calculating average TFIDF scores by category\n",
        "    category_scores = {}\n",
        "    for category, indices in category_indices.items():\n",
        "        category_matrix = tfidf_matrix[indices]\n",
        "        avg_scores = category_matrix.mean(axis=0)\n",
        "        top_words = sorted(\n",
        "            zip(dictionary_list, avg_scores),\n",
        "            key=lambda x: x[1],\n",
        "            reverse=True\n",
        "        )[:3]\n",
        "        category_scores[category] = {word: round(score, 4) for word, score in top_words}\n",
        "\n",
        "    return category_scores\n",
        "\n",
        "# Computing TFIDF scores\n",
        "top_tfidf_words = compute_tfidf_scores_by_category(tfidf_matrix, dataset, dictionary_list)\n",
        "\n",
        "\n",
        "with open('scores.json', 'w') as f:\n",
        "    json.dump(top_tfidf_words, f, indent=4)\n"
      ],
      "metadata": {
        "id": "PsNCttiCzCvY"
      },
      "execution_count": 59,
      "outputs": []
    }
  ]
}